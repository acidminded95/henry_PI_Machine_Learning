{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Henry PI 2: Machine Learning\n",
    "\n",
    "• Caros incluyen el promedio\n",
    "\n",
    "• stacking & walking\n",
    "\n",
    "• revisar descripciones repetidas: anuncios publicados múltiples veces\n",
    "\n",
    "• robustscaler lidia mejor con outliers que standardscaler\n",
    "\n",
    "• el registro más al sur parece estar en Nariño, pero también hay registros en el amazonas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------- D A T A --- E X P L O R A T I O N --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn import preprocessing\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we import the dataset with the training data into a Pandas DataFrame\n",
    "\n",
    "original_df = pd.read_csv('datasets/properties_colombia_train.csv')\n",
    "#original_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we obtain some basic information about the DataFrame, along with the mean value from the feature we will use to create the target column\n",
    "\n",
    "original_price_mean = original_df.price.mean()\n",
    "\n",
    "print(f'• Original shape: {original_df.shape}\\n')\n",
    "print(f'• Original columns: {original_df.columns}\\n')\n",
    "print(f\"• Original price column's mean: {original_price_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for duplicated registers (spoiler: there are none)\n",
    "\n",
    "original_df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for missing values per feature (we find a lot of them, particularly in l4, l5, l6, rooms, bedrooms, surface_total, surface_covered and price_period)\n",
    "\n",
    "original_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------- G E N E R A T I N G --- T A R G E T --- C O L U M N ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a copy of the original dataset and looking for missing values in the 'price' column (which we can see above as well)\n",
    "# Our targets will be obtained from the information contained in this column, so any training data without an associated target value will be pretty much useless.\n",
    "\n",
    "df_Xy = original_df.copy()\n",
    "df_Xy.price.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 63 missing values in the 'price' column, the one we will be using to create our target classification based on it's mean value.\n",
    "\n",
    "We procceed to drop those registers, this is because we need them to have a target value in order to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "price_mean_after_dropna = df_Xy.price.mean()\n",
    "\n",
    "print(f'Original DataFrame Shape: {original_df.shape}')\n",
    "print(f'• DataFrame Shape (after dropna-price): {df_Xy.shape}\\n')\n",
    "print(f'• DataFrame Price column mean (after dropna-price): {price_mean_after_dropna}\\n')\n",
    "print(f\"• Is the price's mean still the same as the original: {price_mean_after_dropna==original_price_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check again for missing values\n",
    "\n",
    "df_Xy.price.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for the extreme values in the column\n",
    "\n",
    "df_Xy.price.min(), df_Xy.price.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check for the amount of appereances of these extreme values\n",
    "\n",
    "df_Xy.price.value_counts()[0], df_Xy.price.value_counts()[345000000000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we found an absurdly big value as the max value of price column we check for some information about the biggest values in this column.\n",
    "\n",
    "#df_Xy.sort_values(by='price',ascending=False).head(100).price.mean() # Output: 54,246'115,351.52\n",
    "#df_Xy.sort_values(by='price',ascending=False).head(1000).price.mean() # Output: 17,829'070,843.313"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • IMPORTANT NOTE:\n",
    "\n",
    "Above we can see that there are extreme outliers in the column from which we are getting our training data targets. This is an important situation that must be adressed with the client, as these outliers (specially the big ones) will distort the column's mean value, affecting the division betweeen 'expensive' and 'cheap' house we are creating in our target column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the 'target' column using the values from 'price', separating them into two categories based on the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy['target'] = (df_Xy['price'] >= original_price_mean).astype(int)\n",
    "print(df_Xy['target'].shape)\n",
    "df_Xy['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look for the amount of different values per feature (in order to filter out redundant and non-informative features)\n",
    "\n",
    "for x in df_Xy:\n",
    "    print(f'\\n• {x}:\\t{len(df_Xy[x].value_counts())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see that:\n",
    "1) There are several features with only one value throughout all of the 197486 registers (ad_type, l1, price_period, operation_type). This features give us no information.\n",
    "2) We can see that the columns labeled 'Unnamed: 0' and the 'id' have unique values (identifiers) for each one of the rows and thus are redundant.\n",
    "\n",
    "We will procceed to create another dataframe from the original one ignoring these features, along with the 'price' column which was only useful for us in order to obtain our 'target' column. \n",
    "\n",
    "After this we will check for duplicates (once we have removed the identifiers that guaranteed every row was unique) and remove them. This will give us a somewhat clean dataset to begin preprocessing our data, i.e. applying to it the changes that we would apply to any input data given to our finished model in order to get predictions from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new DataFrame which we will use to train our model with, ignoring the unnecessary columns\n",
    "\n",
    "df_train = df_Xy.drop(['ad_type', 'l1', 'price_period', 'operation_type', 'Unnamed: 0', 'id', 'price'], axis=1)\n",
    "print(f'• Training DataFrame Shape: {df_train.shape}\\n')\n",
    "print(f'• Training DataFrame Columns: {df_train.columns}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after dropping the redundant and identifier columns we got a total of 4091 duplicated registers.\n",
    "\n",
    "We procceed to eliminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------- D A T A --- P R E P R O C E S S I N G --- 1 --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------- FINDING THE APPROPIATE TRANSFORMATIONS ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will analyze our dataset's features grouping them by the type of data portrayed in them (date, location, ).\n",
    "\n",
    "This way, we will be able to determine the best transformations to perform on each of them in order to feed our models with the best quality data we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'• Total registers: {len(df_train)}')\n",
    "print('• Null values per feature:')\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(feature_list, dataset=df_train, maxmin=False, stats=False):\n",
    "    for x in feature_list:\n",
    "        types = set()\n",
    "        for y in dataset[x]:\n",
    "            types.add(type(y))\n",
    "        print(f'\\n----- {x} -----\\n •Data types: {types}\\n •Missing values:')\n",
    "        print(dataset[x].isnull().value_counts(),'\\n')\n",
    "        if maxmin:\n",
    "            print(f' •Min: {dataset[x].min()}\\n •Max: {dataset[x].max()}\\n')\n",
    "        if stats:\n",
    "            print(f' •Mean: {dataset[x].mean()}\\n •Median: {dataset[x].median()}\\n •Mode: {dataset[x].mode()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) DATE FEATURES: start_date, end_date & created_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_features = ['start_date', 'end_date', 'created_on']\n",
    "\n",
    "get_info(date_features, maxmin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the maximum value for the 'end_date' feature has wrong data, as it is supposed to be the date when the 'for sale' announcement stopped showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.end_date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['start_date', 'end_date', 'created_on']].sort_values(by=['end_date', 'start_date'], ascending=False).head(11928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11925/len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11925 wrong values in the 'end_date' feature (0.06%). \n",
    "\n",
    "We will try to replace them with a the average date difference (between start_date and end_date, not including the wrong values) if we use this column.\n",
    "\n",
    "Also, we will convert this colonms to 'datetime' data type and afterwards wi will format them into timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) LOCATION FEATURES: l2, l3, l4, l5, l6, geometry, lat & lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_features = ['l2', 'l3', 'l4', 'l5', 'l6', 'geometry', 'lat', 'lon']\n",
    "\n",
    "get_info(location_features[:-2])\n",
    "get_info(location_features[-2:], maxmin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see that the 'l4', 'l5' and 'l6' have more than half the values missing, so this columns must be dropped.\n",
    "\n",
    "#### In the 'l2' feature, corresponding to Colombia's departments (their equivalent to states or provinces) we have no values missing.\n",
    "\n",
    "#### In the case of 'l3', there are 10828 values missing (about 5.5% of the registers). We will replace the missing values with the capital of the corresponding departments obtained from 'l2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a dictionary containing each of the 32 colombian departments as keys followed by their corresponding capitals as values.\n",
    "# This list will be stored in the helpers.py file\n",
    "\n",
    "'''\n",
    "capitals = {'Amazonas': 'Leticia', 'Antioquia': 'Medellín', 'Arauca': 'Arauca', 'Atlántico': 'Barranquilla', 'Bolívar' : 'Cartagena', 'Boyacá': 'Tunja',\n",
    "            'Caldas': 'Manizales', 'Caquetá': 'Florencia', 'Casanare': 'Yopal', 'Cauca': 'Popayán', 'Cesar': 'Valledupar', 'Chocó': 'Quibdó', 'Córdoba': 'Montería',\n",
    "            'Cundinamarca': 'Bogotá D.C', 'Guainía': 'Puerto Inírida', 'Guaviare': 'San José del Guaviare', 'Huila': 'Neiva', 'La Guajira': 'Riohacha', \n",
    "            'Magdalena': 'Santa Marta', 'Meta': 'Villavicencio', 'Nariño': 'Pasto', 'Norte de Santander': 'Cúcuta', 'Putumayo': 'Mocoa', 'Quindío': 'Armenia',\n",
    "            'Risaralda': 'Pereira', 'San Andrés Providencia y Santa Catalina': 'San Andrés', 'Santander': 'Bucaramanga', 'Sucre': 'Sincelejo', 'Tolima': 'Ibagué',\n",
    "            'Valle del Cauca': 'Cali', 'Vaupés': 'Mitú', 'Vichada': 'Puerto Carreño'}\n",
    "'''\n",
    "len(capitals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the amount of different cities in the 'l3' feature\n",
    "len(df_train.l3.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding the latitude and longitude values from the dataset, we can see from the output from the function at the beginning of this section that there are 48519 missing values on each of these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check whether the missing values correspond to the same registers in the dataset:\n",
    "\n",
    "print(f\"Rows missing 'lat' values: {len(df_train[df_train['lat'].isnull()])}\")\n",
    "print(f\"Rows missing 'lon' values: {len(df_train[df_train['lon'].isnull()])}\")\n",
    "#print(f\"Rows missing both 'lat' and 'lon' values (1): {len(df_train[df_train['lat'].isnull()][df_train['lon'].isnull()])}\")\n",
    "print(f\"Rows missing both 'lat' and 'lon' values (2): {len(df_train[df_train['lat'].isnull()][df_train[df_train['lat'].isnull()]['lon'].isnull()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take a sample to further proof that every row missing a 'lat' value is missing it's 'lon' value as well\n",
    "df_train[df_train['lat'].isnull()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to analize the latitudes and longitudes from the rows with a value for these columns, we need to define certain limits for the colombian territory, beyond which we shouldn't expect to find any lat or lon values.\n",
    "\n",
    "![Colombia Latitudes and Longitudes](https://i.imgur.com/ZdKWfRG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the corresponding limits as two lists, one for the latitudes and one for the longitudes\n",
    "# This limits encompass the colombian insular territories, which extend further west an north than it's continental territory\n",
    "\n",
    "lat_col = [-4.5, 15]    # Southernmost and northernmost latitudes respectively\n",
    "lon_col = [-82, -67]    # Westernmost and easternmost longitudes respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lat_smaller = 0   # Registers with a latitude to the south of Colombia\n",
    "count_lat_greater = 0   # Registers with a latitude to the north of Colombia\n",
    "for x in df_train.lat:\n",
    "    if x<lat_col[0]:\n",
    "        count_lat_smaller += 1\n",
    "    elif x>lat_col[1]:\n",
    "        count_lat_greater += 1\n",
    "    \n",
    "print(f'• Latitudes south from Colombia: {count_lat_smaller}\\n• Latitudes north from Colombia: {count_lat_greater}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's only 1 value exceeding Colombia's latitudes on each direction in our dataset. We can visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_train.sort_values(by='lat').head(2)\n",
    "#df_Xy.sort_values(by='price',ascending=False).head(100).price.mean() # Output: 54,246'115,351.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by='lat', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lon_smaller = 0   # Registers with a longitude to the west of Colombia\n",
    "count_lon_greater = 0   # Registers with a longitude to the east of Colombia\n",
    "for x in df_train.lon:\n",
    "    if x<lon_col[0]:\n",
    "        count_lon_smaller += 1\n",
    "    elif x>lon_col[1]:\n",
    "        count_lon_greater += 1\n",
    "    \n",
    "print(f'• Longitudes to the west from Colombia: {count_lon_smaller}\\n• Longitudes to the east from Colombia: {count_lon_greater}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found only one missplaced longitude, to the west of Colombia. Now we visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by='lon').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by='lon', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There where only 3 misplaced latitude and longitude values in total. Those can be replaced by the coordinates from the city in the register ('l3' value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get a pd.Series with the possible combinatorics of the values in 'l2' and 'l3' for each row.\n",
    "\n",
    "combinations = []\n",
    "for x in range(len(df_train)):\n",
    "    if str(df_train.iloc[x].l3) != 'nan':\n",
    "        combinations.append(f'{df_train.iloc[x].l3}, {df_train.iloc[x].l2}')\n",
    "comb_series = pd.Series(combinations)\n",
    "unique_l2_l3 = comb_series.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_l2_l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = df_train.l3.unique()\n",
    "print(f'Amount of different cities in df_train.l3: {len(df_cities)}')\n",
    "print(f'Amount of different cities in the combination df_train.l3-df_train.l2: {len(unique_l2_l3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_l3_cities = []\n",
    "repeated_cities = []\n",
    "for x in unique_l2_l3:\n",
    "    y = x.split(',')\n",
    "    if y[0] not in l2_l3_cities:\n",
    "        l2_l3_cities.append(y[0])\n",
    "    else:\n",
    "        repeated_cities.append(y[0])\n",
    "        print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above represents different cities with the same name but in different departments from Colombia. This explains the difference between the amount of unique values in 'l3' and the amount of unique values in the combinatory of 'l2' and 'l3'.\n",
    "\n",
    "Below you can see the complete list of the aforementioned combinatorics and corroborate that the cities listed above have two entries in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in unique_l2_l3:\n",
    "    y = x.split(',')\n",
    "    if y[0] in repeated_cities:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a dictionary with the coordinates for each of the unique combinatorics\n",
    "# This code takes too long to run, so it will be commented out and it's output saved in a dictionary in the helpers.py file.\n",
    "\n",
    "'''\n",
    "geolocator = Nominatim(user_agent='acidminded')\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "dep_ciud_lat_lon = {}\n",
    "\n",
    "for x in capitals.keys():\n",
    "    dep_ciud_lat_lon[x] = {}\n",
    "\n",
    "for x in unique_l2_l3:\n",
    "    coor = geocode(x)\n",
    "    y = x.split(',')\n",
    "    ciud = y[0]\n",
    "    dep = y[1][1:]\n",
    "    dep_ciud_lat_lon[dep][ciud] = {'lat':coor.latitude, 'lon':coor.longitude}\n",
    "\n",
    "for x in capitals:\n",
    "    dep = x\n",
    "    if capitals[x] not in dep_ciud_lat_lon[x]:\n",
    "        ciud = capitals[x]\n",
    "        coor = geocode(f'{ciud}, {dep}')\n",
    "        dep_ciud_lat_lon[dep][ciud] = {'lat':coor.latitude, 'lon':coor.longitude}\n",
    "\n",
    "print(dep_ciud_lat_lon)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check for any problem within our dictionary and found one which is corrected manually.\n",
    "\n",
    "'''\n",
    "count = 0\n",
    "problems = []\n",
    "for dep in dep_ciud_lat_lon:\n",
    "    for city in  dep_ciud_lat_lon[dep]:\n",
    "        if (dep_ciud_lat_lon[dep][city]['lat'] < lat_col[0]) or (dep_ciud_lat_lon[dep][city]['lat'] > lat_col[1]):\n",
    "            count += 1\n",
    "            problems.append((dep, city, 'lat problem'))\n",
    "        if (dep_ciud_lat_lon[dep][city]['lon'] < lon_col[0]) or (dep_ciud_lat_lon[dep][city]['lon'] > lon_col[1]):\n",
    "            count += 1\n",
    "            problems.append((dep, city, 'lon problem'))\n",
    "print(count)\n",
    "print(problems)\n",
    "'''\n",
    "\n",
    "'''\n",
    "OUTPUT:\n",
    "1\n",
    "[('Bolívar', 'Santa Rosa', 'lon problem')]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dep_ciud_lat_lon.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the missing values from the 'geometry' feature\n",
    "\n",
    "df_train.geometry.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 48519 values missing from 'geometry', but once we have obtained the lat and lon for all our missing columns we can fill in this feature as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) PROPERTY FEATURES: rooms, bedrooms, bathrooms, surface_total, surface_covered & property_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_features = ['rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered', 'property_type']\n",
    "\n",
    "get_info(property_features[:-1], maxmin=True, stats=True)\n",
    "get_info(property_features[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above we can see that the only ones of these features that have less than half of it's values missing are 'bathrooms' and 'property_type'.\n",
    "\n",
    "Because of this, 'bathrooms' and 'property_type' will be the only columns from this subset of features that we will be using for training our models by the moment.\n",
    "\n",
    "We are awere that it exists the possibility for us to extract meaningful information from each sale's description in order to fill the missing data from these columns and that may be a path we will explore when improving our first models. But for the moment these two features will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The missing values from the 'bathrooms' column will be imputed with it's floor rounded mean value (2), which also happens to be it's median and mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_values(by=['bathrooms'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_types = df_train.property_type.unique()\n",
    "print(property_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nHouse or apartment registries by amount of bathrooms:\\n')\n",
    "for x in range(5,21):\n",
    "    print(f'• {x} bathrooms:')\n",
    "    print('\\t',len(df_train.loc[((df_train.property_type == 'Casa') | (df_train.property_type == 'Apartamento'))&((df_train.bathrooms >= x))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very unlikely that a house or an apartment will have 6 or more bathrooms, for this reason, those values will be replaced by the floor rounded mean of the column (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) ADVERTISING FEATURES: currency, title & description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_features = ['currency', 'title', 'description']\n",
    "\n",
    "get_info(advertising_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['currency', 'title', 'description']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['currency'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see that 8 of the registers have a price in usd\n",
    "df_train.currency.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train.currency=='USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.description.duplicated().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nd = df_train[['lat', 'lon', 'l2', 'l3', 'l4',\n",
    "       'l5', 'l6', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
    "       'surface_covered', 'currency', 'title', 'description', 'property_type',\n",
    "       'geometry', 'target']].copy()\n",
    "df_train_nd.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nd.description.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nd.drop_duplicates(inplace=True)\n",
    "df_train_nd.description.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_nd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df_train.copy()\n",
    "dft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft[['lat', 'lon', 'l2', 'l3', 'l4',\n",
    "       'l5', 'l6', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
    "       'surface_covered', 'currency', 'title', 'description', 'property_type',\n",
    "       'geometry', 'target']].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.drop_duplicates(subset=['lat', 'lon', 'l2', 'l3', 'l4',\n",
    "       'l5', 'l6', 'rooms', 'bedrooms', 'bathrooms', 'surface_total',\n",
    "       'surface_covered', 'currency', 'title', 'description', 'property_type',\n",
    "       'geometry', 'target'], inplace=True)\n",
    "dft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dft.surface_total.isnull().value_counts()\n",
    "#dft.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------- D A T A --- P R E P R O C E S S I N G --- 2 --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------ CREATING THE PIPELINE ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will design a pipeline the recieves a dataset with the same features as the one we just explored ('df_train') as it was at the beginning of the previous section (minus the target column). This pipeline will perform the necessary changes to the dataset, feed it to a model of our selection, perform a cross validation and give us the results.\n",
    "\n",
    "As we concluded on the previous section, we will select a few features that will be considered relevant to continue with the data preprocessing and model training: l2, l3, lat, lon, bathrooms and property_type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2--- categorical (needs encoding). MissVal (0, ok!)\n",
    "\n",
    "l3--- categorical (needs encoding). MissVal (needs imputation using 'capitals')\n",
    "\n",
    "lat--- numerical (ok). MissVal (needs imputation using 'dep_ciud_lat_lon') -standard scaler\n",
    "\n",
    "lon--- numerical (ok). MissVal (needs imputation using 'dep_ciud_lat_lon')\n",
    "\n",
    "bathrooms-- numerical (ok). MissVal (needs imputation using mean 2). Replace values greater than 5 (and with property type 'casa' or 'apartamento) with 2 by 2\n",
    "\n",
    "property_type--- categorical (needs encoding). MissVal (0, ok!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dft.drop('target',axis=1)\n",
    "y1 = dft.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define some helper functions that will be used to fill missing values during the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_l3(df):\n",
    "    l3_ok = []\n",
    "    for x in range(len(df)):\n",
    "        if type(df.loc[x,'l3']) == float:\n",
    "            dep = df.loc[x,'l2']\n",
    "            l3_ok.append(str(capitals[dep]))\n",
    "        else:\n",
    "            l3_ok.append(str(df.loc[x,'l3']))\n",
    "    return pd.Series(l3_ok)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_coor(df):\n",
    "    lat_ok = []\n",
    "    lon_ok = []\n",
    "    for x in range(len(df)):\n",
    "        if str(df.loc[x,'lat']) == 'nan':\n",
    "            dep, city = df.loc[x,'l2':'l3']\n",
    "            #print('NAN FOUND: ',dep, city, df.loc[x,'lat'], df.loc[x,'lon'])\n",
    "            try:\n",
    "                lat_ok.append(float(dep_ciud_lat_lon[dep][city]['lat']))\n",
    "                lon_ok.append(float(dep_ciud_lat_lon[dep][city]['lon']))\n",
    "            except KeyError:\n",
    "                geolocator = Nominatim(user_agent='acidminded')\n",
    "                geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "                txt = f'{city}, {dep}'\n",
    "                coor = geocode(txt)\n",
    "                lat_ok.append(coor.latitude)\n",
    "                lon_ok.append(coor.longitude)\n",
    "                dep_ciud_lat_lon[dep][city] = {'lat':coor.latitude, 'lon':coor.longitude}\n",
    "                print(f'NEW DATA: {dep}{city} lat: {coor.latitude} lon: {coor.longitude}')\n",
    "        else:\n",
    "            lat_ok.append(df.loc[x,'lat'])\n",
    "            lon_ok.append(df.loc[x,'lon'])\n",
    "    return pd.Series(lat_ok), pd.Series(lon_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prtypes = ['Casa', 'Apartamento', 'Otro', 'Oficina', 'Finca', 'Lote', 'Local comercial', 'Parqueadero']\n",
    "prtypes_avgbtrms = {}\n",
    "\n",
    "for x in prtypes:\n",
    "    prtypes_avgbtrms[x] = X1.loc[X1.property_type==x].bathrooms.mean()\n",
    "    if str(prtypes_avgbtrms[x]) == 'nan':\n",
    "        prtypes_avgbtrms[x] = 1\n",
    "for x in prtypes_avgbtrms:\n",
    "    prtypes_avgbtrms[x] = round(prtypes_avgbtrms[x])\n",
    "\n",
    "def fill_bathrooms(df):  #hehe\n",
    "    btrms_ok = []\n",
    "    for x in range(len(df)):\n",
    "        btrms = df.loc[x,'bathrooms'] \n",
    "        prtype = df.loc[x,'property_type']\n",
    "\n",
    "        if str(btrms) == 'nan':\n",
    "            btrms_ok.append(prtypes_avgbtrms[prtype])\n",
    "        elif btrms >= 6:\n",
    "            if prtype in ['Casa', 'Apartamento']:\n",
    "                btrms_ok.append(float(2))\n",
    "            else:\n",
    "                btrms_ok.append(btrms)\n",
    "        else:\n",
    "            btrms_ok.append(btrms)\n",
    "    return pd.Series(btrms_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std_scaler = StandardScaler().fit(X1[['lat','lon']].to_numpy()[:,:])\n",
    "#min_max_scaler = MinMaxScaler().fit(X1[['bathrooms']].to_numpy()[:,:])\n",
    "print(X1.to_numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(dataset):\n",
    "    X = dataset[['bathrooms', 'lat', 'lon', 'l2', 'l3', 'property_type']].copy()\n",
    "    X.reset_index(inplace=True, drop=True)\n",
    "    # Fill missing values\n",
    "    X['l3'] = fill_l3(X)\n",
    "    X['lat'], X['lon'] = fill_coor(X)\n",
    "    X['bathrooms'] = fill_bathrooms(X)\n",
    "    # Prepare to transform to numerical\n",
    "    X['l2'] = X['l2'].astype('category')\n",
    "    X['l3'] = X['l3'].astype('category')\n",
    "    X['property_type'] = X['property_type'].astype('category')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = fill_nan(X1)\n",
    "X2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_coder = OneHotEncoder()\n",
    "l3_coder = OneHotEncoder()\n",
    "pt_coder = OneHotEncoder()\n",
    "depts = capitals.keys()\n",
    "cities = set()\n",
    "for dept in dep_ciud_lat_lon:\n",
    "    for city in dep_ciud_lat_lon[dept]:\n",
    "        cities.add(city)\n",
    "\n",
    "l2_cod = l2_coder.fit(pd.Series(depts).values.reshape(-1, 1))\n",
    "l3_cod = l3_coder.fit(pd.Series(list(cities)).values.reshape(-1, 1))\n",
    "pt_cod = pt_coder.fit(X2[['property_type']])\n",
    "'''\n",
    "l2_cod = l2_coder.fit(X2[['l2']])\n",
    "l3_cod = l3_coder.fit(X2[['l3']])\n",
    "pt_cod = pt_coder.fit(X2[['property_type']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_num(dataset):\n",
    "    #print(f'INITIAL TO_NUM SHAPE: {dataset.shape}')\n",
    "    \n",
    "    X = dataset.copy()\n",
    "    \n",
    "    #print(f'TO_NUM SHAPE BEFORE ONEHOT: {X.shape}')\n",
    "\n",
    "    #l2_coder = OneHotEncoder()\n",
    "    #l2_cod = l2_coder.fit_transform(X[['l2']])\n",
    "    l2_cod = l2_coder.transform(X[['l2']])\n",
    "    new_l2 = pd.DataFrame(l2_cod.toarray())\n",
    "    #print(f'+{new_l2.shape} (l2)')\n",
    "\n",
    "    #l3_coder = OneHotEncoder()\n",
    "    #l3_cod = l3_coder.fit_transform(X[['l3']])\n",
    "    l3_cod = l3_coder.transform(X[['l3']])\n",
    "    new_l3 = pd.DataFrame(l3_cod.toarray())\n",
    "    #print(f'+{new_l3.shape} (l3)')\n",
    "\n",
    "    #pt_coder = OneHotEncoder()\n",
    "    #pt_cod = pt_coder.fit_transform(X[['property_type']])\n",
    "    pt_cod = pt_coder.transform(X[['property_type']])\n",
    "    new_pt = pd.DataFrame(pt_cod.toarray(), columns=pt_coder.categories_)\n",
    "    #print(f'+{new_pt.shape} (pt)')\n",
    "\n",
    "    #print(f'TO_NUM SHAPE AFTER ONEHOT: {X.shape}')\n",
    "\n",
    "    X_ok = pd.concat([X, new_l2, new_l3, new_pt], axis=1)\n",
    "    X_ok.drop(['l2','l3','property_type'], axis=1, inplace=True)\n",
    "\n",
    "    X_num = X_ok.to_numpy()\n",
    "    #print(f'FINAL TO_NUM SHAPE: {X_num.shape}')\n",
    "\n",
    "    return X_num\n",
    "    \n",
    "\n",
    "X_num = preprocess_to_num(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_num.shape)\n",
    "X_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler().fit(X_num[:,:1])\n",
    "std_scaler = StandardScaler().fit(X_num[:,1:])\n",
    "pca = PCA(n_components=30, whiten=False).fit(X_num)\n",
    "\n",
    "def preprocess_std_dimred(matrix):\n",
    "    #print(f' STANDARDIZATION INITIAL SHAPE: {matrix.shape}')\n",
    "    X =  np.copy(matrix)\n",
    "    X[:,:1] = min_max_scaler.transform(matrix[:,:1])\n",
    "    X[:,1:] = std_scaler.transform(matrix[:,1:])\n",
    "    #print(f' STANDARDIZATION INTERMEDIATE SHAPE: {X.shape}')\n",
    "    X = pca.transform(X)\n",
    "    #print(f' STANDARDIZATION FINAL SHAPE: {X.shape}')\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ready = preprocess_std_dimred(X_num)\n",
    "X_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_df = FunctionTransformer(fill_nan)\n",
    "df_to_num = FunctionTransformer(preprocess_to_num)\n",
    "mat_to_X = FunctionTransformer(preprocess_std_dimred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df_ok = df.copy()\n",
    "    df_ok2 = fill_nan(df_ok)\n",
    "    df_ok3 = preprocess_to_num(df_ok2)\n",
    "    df_ok4 = preprocess_std_dimred(df_ok3)\n",
    "\n",
    "    return df_ok4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_print(pipeline, X_train, y_train, X_test, y_test):\n",
    "    #print(f'SHAPE TO FIT: {X_train.shape}')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds = pipeline.predict(X_test)\n",
    "    print('• TRAIN DATA:')\n",
    "    print(f'Train confusion matrix: \\n{confusion_matrix(y_train, train_preds)}')\n",
    "    print(f'Train accuracy: {accuracy_score(y_train, train_preds)}')\n",
    "    print(f'Train recall: {recall_score(y_train, train_preds)}\\n\\n')\n",
    "\n",
    "    print('• TEST DATA:')\n",
    "    print(f'Test confusion matrix: \\n{confusion_matrix(y_test, test_preds)}')\n",
    "    print(f'Test accuracy: {accuracy_score(y_test, test_preds)}')\n",
    "    print(f'Test recall: {recall_score(y_test, test_preds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_1 = Pipeline([('Fill_DF', fill_df),('To_Num', df_to_num),('Standardize_&_DimRed', mat_to_X), ('KNClassifier-10nneigh', KNeighborsClassifier(n_neighbors=10))])\n",
    "pipeline_2 = Pipeline([('Fill_DF', fill_df),('To_Num', df_to_num),('Standardize_&_DimRed', mat_to_X), ('KNClassifier-15nneigh', KNeighborsClassifier(n_neighbors=15))])\n",
    "pipeline_3 = Pipeline([('Fill_DF', fill_df),('To_Num', df_to_num),('Standardize_&_DimRed', mat_to_X), ('DTClassifier-mxdepth8', DecisionTreeClassifier(max_depth=8))])\n",
    "pipeline_4 = Pipeline([('Fill_DF', fill_df),('To_Num', df_to_num),('Standardize_&_DimRed', mat_to_X), ('DTClassifier-mxdepth15', DecisionTreeClassifier(max_depth=15))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y1, test_size=0.2, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneighbors classifier n_neighbors=10\n",
    "fit_and_print(pipeline_1, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneighbors classifier n_neighbors=15\n",
    "fit_and_print(pipeline_2, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier max_depth=8\n",
    "fit_and_print(pipeline_3, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier max_depth=15\n",
    "fit_and_print(pipeline_4, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test = pd.read_csv('datasets/properties_colombia_test.csv')\n",
    "real_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_X = preprocess(real_test)\n",
    "real_test_X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
